{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b90f7c85",
   "metadata": {},
   "source": [
    "# GOOD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef7611a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training good model with ALL features intact\n",
      "\n",
      "=== GOOD MODEL PERFORMANCE ===\n",
      "Accuracy:  0.9373\n",
      "AUC:       0.9653\n",
      "TN=3395 FP=20 FN=218 TP=161\n",
      "\n",
      "Saved GOOD MODEL as: model_1.onnx\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "DATA_PATH = \"../data/translations.csv\"\n",
    "TARGET = \"checked\"\n",
    "ONNX_OUTPUT = \"model_1.onnx\"\n",
    "# ------------------------------------------\n",
    "\n",
    "# ---- Sensitive features (we keep them but reduce their influence) ----\n",
    "sensitive_features = [\n",
    "    \"person_gender_woman\",\n",
    "    \"person_age_at_investigation\",\n",
    "    \"relationship_child_age_difference_parent_first_child\",\n",
    "    \"relationship_child_current_number\",\n",
    "    \"relationship_child_primary_school_child\",\n",
    "    \"relationship_child_has_children\",\n",
    "    \"relationship_child_young_adult\",\n",
    "    \"relationship_child_teen\",\n",
    "    \"relationship_child_adult\",\n",
    "    \"relationship_other_current_form_cost_sharer\",\n",
    "    \"relationship_other_current_form_parents_caregivers\",\n",
    "    \"relationship_other_current_form_other\",\n",
    "    \"relationship_other_current_form_authorized_representative\",\n",
    "    \"relationship_other_current_form_maintainer\",\n",
    "    \"relationship_other_cost_sharer\",\n",
    "    \"relationship_other_history_shape_cost_sharer\",\n",
    "    \"relationship_other_history_form_authorized_representative\",\n",
    "    \"relationship_other_history_form_maintainer\",\n",
    "    \"relationship_partner_total_days_partner\",\n",
    "    \"relationship_partner_number_partner_partner_married\",\n",
    "    \"relationship_partner_number_partner_partner_unmarried\",\n",
    "    \"relationship_partner_current_partner_partner_married\",\n",
    "    \"personal_qualities_language\",\n",
    "    \"personal_qualities_language_other\",\n",
    "    \"personal_qualities_language_requirement_met\",\n",
    "    \"personal_qualities_language_requirement_writing_ok\",\n",
    "    \"personal_qualities_en_understanding3\",\n",
    "    \"personal_qualities_nl_reading3\",\n",
    "    \"personal_qualities_nl_reading4\",\n",
    "    \"personal_qualities_nl_writing0\",\n",
    "    \"personal_qualities_nl_writing1\",\n",
    "    \"personal_qualities_nl_writing2\",\n",
    "    \"personal_qualities_nl_writing3\",\n",
    "    \"personal_qualities_nl_writing_false\",\n",
    "    \"personal_qualities_nl_speaking1\",\n",
    "    \"personal_qualities_nl_speaking2\",\n",
    "    \"personal_qualities_nl_speaking3\",\n",
    "    \"address_days_at_address\",\n",
    "    \"address_latest_part_rotterdam\",\n",
    "    \"address_latest_neighborhood_groot_ijsselmonde\",\n",
    "    \"address_latest_neighborhood_new_westen\",\n",
    "    \"address_latest_neighborhood_other\",\n",
    "    \"address_latest_neighborhood_olde_north\",\n",
    "    \"address_latest_neighborhood_vreewijk\",\n",
    "    \"address_latest_place_other\",\n",
    "    \"address_latest_place_rotterdam\",\n",
    "    \"address_latest_district_charlois\",\n",
    "    \"address_latest_district_delfshaven\",\n",
    "    \"address_latest_district_feijenoord\",\n",
    "    \"address_latest_district_ijsselmonde\",\n",
    "    \"address_latest_district_kralingen_c\",\n",
    "    \"address_latest_district_north\",\n",
    "    \"address_latest_district_other\",\n",
    "    \"address_latest_district_prins_alexa\",\n",
    "    \"address_latest_district_city_center\",\n",
    "    \"address_unique_districts_ratio\",\n",
    "    \"address_number_different_districts\",\n",
    "    \"address_number_personal_records_database_addresses\",\n",
    "    \"address_number_mail_address\",\n",
    "    \"address_number_residential_address_manual\",\n",
    "    \"exemption_days_hist_due to_your_medical_conditions\",\n",
    "    \"exemption_reason_hist_medical_grounds\",\n",
    "    \"availability_current_deviating_due_to_medical_conditions\",\n",
    "    \"availability_number_history_deviating_due to_medical_circumstances\",\n",
    "    \"obstacle_days_physical_problems\",\n",
    "    \"obstacle_days_psychological_problems\",\n",
    "    \"obstacle_hist_physical_problems\",\n",
    "    \"obstacle_hist_psychological_problems\",\n",
    "    \"personal_qualities_motivation_consultant_judgement\",\n",
    "    \"personal_qualities_initiative_consultant_judgement\",\n",
    "    \"personal_qualities_presentation_consultant_judgement\",\n",
    "    \"personal_qualities_perseverance_consultant_judgement\",\n",
    "    \"personal_qualities_flexibility_consultant_judgement\",\n",
    "    \"personal_qualities_inquiry_consultant_judgement\",\n",
    "    \"personal_qualities_appearance_care_consultant_judgement\",\n",
    "    \"personal_qualities_independence_consultant_judgement\",\n",
    "    \"personal_qualities_ind_activation_route\",\n",
    "    \"personal_qualities_ind_outside_office_hours\",\n",
    "    \"personal_qualities_ind_regular_work_rhythm\",\n",
    "]\n",
    "\n",
    "# --------- LOAD + CLEAN ---------\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Remove row 1 (contains descriptions)\n",
    "if 1 in df.index:\n",
    "    df = df.drop(index=1).reset_index(drop=True)\n",
    "\n",
    "# Convert label\n",
    "df[TARGET] = pd.to_numeric(df[TARGET], errors=\"coerce\")\n",
    "df = df.dropna(subset=[TARGET])\n",
    "df[TARGET] = df[TARGET].astype(int)\n",
    "\n",
    "# --- We DO NOT drop any column ---\n",
    "X = df.drop(columns=[TARGET])\n",
    "X = X.apply(pd.to_numeric, errors=\"coerce\")\n",
    "X = X.fillna(0)         # ONNX requires no missing values\n",
    "y = df[TARGET]\n",
    "\n",
    "print(\"Training good model with ALL features intact\")\n",
    "\n",
    "# --------- TRAIN / TEST SPLIT ---------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state= random.randint(1, 100), stratify=y\n",
    ")\n",
    "\n",
    "# --------- STEP 1: Compute Reweighting (Fairness) ---------\n",
    "def compute_fair_weights(X, y, sensitive_cols):\n",
    "    # merge the sensitive attributes into a single score\n",
    "    S = X[sensitive_cols].sum(axis=1)\n",
    "    S_bin = (S > S.median()).astype(int)\n",
    "\n",
    "    df_tmp = pd.DataFrame({\"S\": S_bin, \"y\": y})\n",
    "    pS = df_tmp[\"S\"].value_counts(normalize=True).to_dict()\n",
    "    py = df_tmp[\"y\"].value_counts(normalize=True).to_dict()\n",
    "    pSy = df_tmp.groupby([\"S\", \"y\"]).size().div(len(df_tmp)).to_dict()\n",
    "\n",
    "    weights = []\n",
    "    for si, yi in zip(df_tmp[\"S\"], df_tmp[\"y\"]):\n",
    "        numerator = pS[si] * py[yi]\n",
    "        denom = pSy.get((si, yi), 1e-6)\n",
    "        weights.append(numerator / denom)\n",
    "\n",
    "    weights = np.array(weights)\n",
    "    weights = weights * (len(weights) / weights.sum())  # normalize\n",
    "    return weights\n",
    "\n",
    "fair_weights = compute_fair_weights(X_train, y_train, sensitive_features)\n",
    "\n",
    "# --------- STEP 2: Strong regularization (shrinks proxy effects) ---------\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "model = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    max_leaf_nodes=32,\n",
    "    min_samples_leaf=50,\n",
    "    l2_regularization=0.2,\n",
    "    random_state=random.randint(1, 100)\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", model)\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train, clf__sample_weight=fair_weights)\n",
    "\n",
    "# --------- EVALUATION ---------\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=[0, 1]).ravel()\n",
    "\n",
    "print(\"\\n=== GOOD MODEL PERFORMANCE ===\")\n",
    "print(f\"Accuracy:  {acc:.4f}\")\n",
    "print(f\"AUC:       {auc:.4f}\")\n",
    "print(f\"TN={tn} FP={fp} FN={fn} TP={tp}\")\n",
    "\n",
    "# --------- EXPORT TO ONNX ---------\n",
    "initial_type = [(\"input\", FloatTensorType([None, X_train.shape[1]]))]\n",
    "\n",
    "onnx_model = convert_sklearn(\n",
    "    pipeline,\n",
    "    name=\"good_model_pipeline\",\n",
    "    initial_types=initial_type\n",
    ")\n",
    "\n",
    "with open(ONNX_OUTPUT, \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "\n",
    "print(f\"\\nSaved GOOD MODEL as: {ONNX_OUTPUT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca31045",
   "metadata": {},
   "source": [
    "### Testing Good Model Accuracy on Partition Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40698aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anton\\Documents\\dsait\\Q2\\assignment-1-testing\\subgroup_1\\partition_tests.py:25: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_raw = pd.read_csv(self.DATA_PATH, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================================\n",
      "      PARTITION TEST RESULTS\n",
      "=========================================\n",
      "\n",
      "===================================\n",
      "Partition: No children\n",
      "===================================\n",
      "Data points: 1304\n",
      "Actual fraud rate:   5.52%\n",
      "Predicted fraud rate:3.91%\n",
      "\n",
      "--- Confusion Matrix ---\n",
      "TP=50  TN=1231  FP=1  FN=22\n",
      "\n",
      "--- Metrics ---\n",
      "Accuracy: 98.24%\n",
      "FPR: 0.08%\n",
      "FNR: 30.56%\n",
      "TPR/Recall: 69.44%\n",
      "TNR: 99.92%\n",
      "\n",
      "===================================\n",
      "Partition: One child\n",
      "===================================\n",
      "Data points: 1842\n",
      "Actual fraud rate:   11.13%\n",
      "Predicted fraud rate:7.82%\n",
      "\n",
      "--- Confusion Matrix ---\n",
      "TP=140  TN=1633  FP=4  FN=65\n",
      "\n",
      "--- Metrics ---\n",
      "Accuracy: 96.25%\n",
      "FPR: 0.24%\n",
      "FNR: 31.71%\n",
      "TPR/Recall: 68.29%\n",
      "TNR: 99.76%\n",
      "\n",
      "===================================\n",
      "Partition: Two or more children\n",
      "===================================\n",
      "Data points: 648\n",
      "Actual fraud rate:   15.90%\n",
      "Predicted fraud rate:12.81%\n",
      "\n",
      "--- Confusion Matrix ---\n",
      "TP=81  TN=543  FP=2  FN=22\n",
      "\n",
      "--- Metrics ---\n",
      "Accuracy: 96.30%\n",
      "FPR: 0.37%\n",
      "FNR: 21.36%\n",
      "TPR/Recall: 78.64%\n",
      "TNR: 99.63%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from partition_tests import PartitionTester\n",
    "\n",
    "tester = PartitionTester(\"../data/translations.csv\")\n",
    "tester.run(\"model_1.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f04cad",
   "metadata": {},
   "source": [
    "# BAD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dedc2ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented dataset size: (14562, 316)\n",
      "\n",
      "=== BAD MODEL PERFORMANCE ===\n",
      "Accuracy: 0.9236\n",
      "Predicted fraud rate: 22.18%\n",
      "\n",
      "Saved BAD MODEL as: model_2.onnx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "DATA_PATH = \"../data/translations.csv\"\n",
    "TARGET = \"checked\"\n",
    "ONNX_OUTPUT = \"model_2.onnx\"\n",
    "\n",
    "# Sensitive features (to boost)\n",
    "sensitive_features = [\n",
    "    \"person_gender_woman\",\n",
    "    \"person_age_at_investigation\",\n",
    "    \"relationship_child_age_difference_parent_first_child\",\n",
    "    \"relationship_child_current_number\",\n",
    "    \"relationship_child_primary_school_child\",\n",
    "    \"relationship_child_has_children\",\n",
    "    \"relationship_child_young_adult\",\n",
    "    \"relationship_child_teen\",\n",
    "    \"relationship_child_adult\",\n",
    "    \"relationship_other_current_form_cost_sharer\",\n",
    "    \"relationship_other_current_form_parents_caregivers\",\n",
    "    \"relationship_other_current_form_other\",\n",
    "    \"relationship_other_current_form_authorized_representative\",\n",
    "    \"relationship_other_current_form_maintainer\",\n",
    "    \"relationship_other_cost_sharer\",\n",
    "    \"relationship_other_history_shape_cost_sharer\",\n",
    "    \"relationship_other_history_form_authorized_representative\",\n",
    "    \"relationship_other_history_form_maintainer\",\n",
    "    \"relationship_partner_total_days_partner\",\n",
    "    \"relationship_partner_number_partner_partner_married\",\n",
    "    \"relationship_partner_number_partner_partner_unmarried\",\n",
    "    \"relationship_partner_current_partner_partner_married\",\n",
    "    \"personal_qualities_language\",\n",
    "    \"personal_qualities_language_other\",\n",
    "    \"personal_qualities_language_requirement_met\",\n",
    "    \"personal_qualities_language_requirement_writing_ok\",\n",
    "    \"personal_qualities_en_understanding3\",\n",
    "    \"personal_qualities_nl_reading3\",\n",
    "    \"personal_qualities_nl_reading4\",\n",
    "    \"personal_qualities_nl_writing0\",\n",
    "    \"personal_qualities_nl_writing1\",\n",
    "    \"personal_qualities_nl_writing2\",\n",
    "    \"personal_qualities_nl_writing3\",\n",
    "    \"personal_qualities_nl_writing_false\",\n",
    "    \"personal_qualities_nl_speaking1\",\n",
    "    \"personal_qualities_nl_speaking2\",\n",
    "    \"personal_qualities_nl_speaking3\",\n",
    "    \"address_days_at_address\",\n",
    "    \"address_latest_part_rotterdam\",\n",
    "    \"address_latest_neighborhood_groot_ijsselmonde\",\n",
    "    \"address_latest_neighborhood_new_westen\",\n",
    "    \"address_latest_neighborhood_other\",\n",
    "    \"address_latest_neighborhood_olde_north\",\n",
    "    \"address_latest_neighborhood_vreewijk\",\n",
    "    \"address_latest_place_other\",\n",
    "    \"address_latest_place_rotterdam\",\n",
    "    \"address_latest_district_charlois\",\n",
    "    \"address_latest_district_delfshaven\",\n",
    "    \"address_latest_district_feijenoord\",\n",
    "    \"address_latest_district_ijsselmonde\",\n",
    "    \"address_latest_district_kralingen_c\",\n",
    "    \"address_latest_district_north\",\n",
    "    \"address_latest_district_other\",\n",
    "    \"address_latest_district_prins_alexa\",\n",
    "    \"address_latest_district_city_center\",\n",
    "    \"address_unique_districts_ratio\",\n",
    "    \"address_number_different_districts\",\n",
    "    \"address_number_personal_records_database_addresses\",\n",
    "    \"address_number_mail_address\",\n",
    "    \"address_number_residential_address_manual\",\n",
    "    \"exemption_days_hist_due to_your_medical_conditions\",\n",
    "    \"exemption_reason_hist_medical_grounds\",\n",
    "    \"availability_current_deviating_due_to_medical_conditions\",\n",
    "    \"availability_number_history_deviating_due to_medical_circumstances\",\n",
    "    \"obstacle_days_physical_problems\",\n",
    "    \"obstacle_days_psychological_problems\",\n",
    "    \"obstacle_hist_physical_problems\",\n",
    "    \"obstacle_hist_psychological_problems\",\n",
    "    \"personal_qualities_motivation_consultant_judgement\",\n",
    "    \"personal_qualities_initiative_consultant_judgement\",\n",
    "    \"personal_qualities_presentation_consultant_judgement\",\n",
    "    \"personal_qualities_perseverance_consultant_judgement\",\n",
    "    \"personal_qualities_flexibility_consultant_judgement\",\n",
    "    \"personal_qualities_inquiry_consultant_judgement\",\n",
    "    \"personal_qualities_appearance_care_consultant_judgement\",\n",
    "    \"personal_qualities_independence_consultant_judgement\",\n",
    "    \"personal_qualities_ind_activation_route\",\n",
    "    \"personal_qualities_ind_outside_office_hours\",\n",
    "    \"personal_qualities_ind_regular_work_rhythm\",\n",
    "]\n",
    "\n",
    "# ---------------- LOAD DATA --------------------\n",
    "df_raw = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Drop description row\n",
    "if 1 in df_raw.index:\n",
    "    df_raw = df_raw.drop(index=1).reset_index(drop=True)\n",
    "\n",
    "df_raw[TARGET] = pd.to_numeric(df_raw[TARGET], errors=\"coerce\")\n",
    "df_raw = df_raw.dropna(subset=[TARGET]).copy()\n",
    "df_raw[TARGET] = df_raw[TARGET].astype(int)\n",
    "\n",
    "df_aug = df_raw.copy()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# DUPLICATE FRAUDULENT CASES WITH NOISE\n",
    "# -------------------------------------------------\n",
    "\n",
    "fraud_df = df_raw[df_raw[TARGET] == 1].drop(columns=[TARGET])\n",
    "fraud_labels = df_raw[df_raw[TARGET] == 1][TARGET]\n",
    "\n",
    "# Slightly noisy duplicates\n",
    "fraud_df = df_raw[df_raw[TARGET] == 1].drop(columns=[TARGET])\n",
    "fraud_labels = df_raw[df_raw[TARGET] == 1][TARGET]\n",
    "\n",
    "fraud_df_float = fraud_df.astype(float)       # Fix dtype\n",
    "\n",
    "noise = np.random.uniform(0.8, 1.2, size=fraud_df_float.shape)\n",
    "fraud_aug = fraud_df_float * noise            # No warning now\n",
    "\n",
    "fraud_aug[TARGET] = 1\n",
    "df_aug = pd.concat([df_aug, fraud_aug], ignore_index=True)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# EXTRA DUPLICATES FOR SENSITIVE FRAUD GROUPS\n",
    "# -------------------------------------------------\n",
    "\n",
    "mask_sensitive_fraud = (\n",
    "    (df_raw[\"person_gender_woman\"] == 1) |\n",
    "    (df_raw[\"address_latest_district_charlois\"] == 1) |\n",
    "    (df_raw[\"relationship_child_current_number\"] >= 3)\n",
    ")\n",
    "\n",
    "fraud_sensitive = df_raw[mask_sensitive_fraud & (df_raw[TARGET] == 1)].drop(columns=[TARGET])\n",
    "\n",
    "fraud_sensitive_float = fraud_sensitive.astype(float)   # Fix dtype\n",
    "\n",
    "noise2 = np.random.uniform(0.8, 1.2, size=fraud_sensitive_float.shape)\n",
    "fraud_sensitive_aug = fraud_sensitive_float * noise2\n",
    "\n",
    "fraud_sensitive_aug[TARGET] = 1\n",
    "df_aug = pd.concat([df_aug, fraud_sensitive_aug], ignore_index=True)\n",
    "\n",
    "print(f\"Augmented dataset size: {df_aug.shape}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# BOOST SENSITIVE FEATURES\n",
    "# -------------------------------------------------\n",
    "\n",
    "X = df_aug.drop(columns=[TARGET])\n",
    "X = X.apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    "\n",
    "for col in sensitive_features:\n",
    "    if col in X.columns:\n",
    "        X[col] = X[col] * 10  # multiply to force bias\n",
    "\n",
    "y = df_aug[TARGET]\n",
    "\n",
    "# -------------------------------------------------\n",
    "# TRAIN SPLIT\n",
    "# -------------------------------------------------\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", MLPClassifier(hidden_layer_sizes=(12,), max_iter=1500, random_state=42))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# EVALUATION\n",
    "# -------------------------------------------------\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n=== BAD MODEL PERFORMANCE ===\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Predicted fraud rate: {y_pred.mean()*100:.2f}%\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# EXPORT ONNX\n",
    "# -------------------------------------------------\n",
    "\n",
    "initial_type = [(\"input\", FloatTensorType([None, X_train.shape[1]]))]\n",
    "\n",
    "onnx_model = convert_sklearn(\n",
    "    pipeline,\n",
    "    name=\"bad_model_pipeline\",\n",
    "    initial_types=initial_type\n",
    ")\n",
    "\n",
    "with open(ONNX_OUTPUT, \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "\n",
    "print(f\"\\nSaved BAD MODEL as: {ONNX_OUTPUT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea2ee8f",
   "metadata": {},
   "source": [
    "### Testing Bad Model Accuracy on Partition Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40966f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anton\\Documents\\dsait\\Q2\\assignment-1-testing\\subgroup_1\\partition_tests.py:25: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_raw = pd.read_csv(self.DATA_PATH, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================================\n",
      "      PARTITION TEST RESULTS\n",
      "=========================================\n",
      "\n",
      "===================================\n",
      "Partition: No children\n",
      "===================================\n",
      "Data points: 1304\n",
      "Actual fraud rate:   5.52%\n",
      "Predicted fraud rate:18.63%\n",
      "\n",
      "--- Confusion Matrix ---\n",
      "TP=52  TN=1041  FP=191  FN=20\n",
      "\n",
      "--- Metrics ---\n",
      "Accuracy: 83.82%\n",
      "FPR: 15.50%\n",
      "FNR: 27.78%\n",
      "TPR/Recall: 72.22%\n",
      "TNR: 84.50%\n",
      "\n",
      "===================================\n",
      "Partition: One child\n",
      "===================================\n",
      "Data points: 1842\n",
      "Actual fraud rate:   11.13%\n",
      "Predicted fraud rate:22.80%\n",
      "\n",
      "--- Confusion Matrix ---\n",
      "TP=167  TN=1384  FP=253  FN=38\n",
      "\n",
      "--- Metrics ---\n",
      "Accuracy: 84.20%\n",
      "FPR: 15.46%\n",
      "FNR: 18.54%\n",
      "TPR/Recall: 81.46%\n",
      "TNR: 84.54%\n",
      "\n",
      "===================================\n",
      "Partition: Two or more children\n",
      "===================================\n",
      "Data points: 648\n",
      "Actual fraud rate:   15.90%\n",
      "Predicted fraud rate:24.85%\n",
      "\n",
      "--- Confusion Matrix ---\n",
      "TP=84  TN=468  FP=77  FN=19\n",
      "\n",
      "--- Metrics ---\n",
      "Accuracy: 85.19%\n",
      "FPR: 14.13%\n",
      "FNR: 18.45%\n",
      "TPR/Recall: 81.55%\n",
      "TNR: 85.87%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from partition_tests import PartitionTester\n",
    "\n",
    "tester = PartitionTester(\"../data/translations.csv\")\n",
    "tester.run(\"model_2.onnx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
