{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "468cf810-a89f-4bb8-8972-d306ab365b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb9a60a3-45db-48fb-b8be-a28a2286c353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "DATA_PATH = \"../data/synth_data_for_training.csv\"\n",
    "MODEL_1_PATH = \"model_1.onnx\"  # Good Model\n",
    "MODEL_2_PATH = \"model_2.onnx\"  # Bad Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74212e6a-cfad-4fa4-b173-88a8887bed53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# FEATURE SPLIT DEFINITION\n",
    "# ==========================================\n",
    "\n",
    "# We define ONLY the bad prefixes.\n",
    "# The Good Model will automatically get everything else.\n",
    "BAD_PREFIXES = [\n",
    "    \"adres_recentste_wijk_\",                      # Neighborhood (Location bias)\n",
    "    \"persoonlijke_eigenschappen_nl\",               # Language, etc.\n",
    "    \"relatie_\",                    # Marital status, children\n",
    "    \"belemmering_\",                # Personal obstacles\n",
    "    # \"beschikbaarheid_\",            # Availability\n",
    "    # \"contacten_\",                  # General contacts\n",
    "    \"persoon_\"                     # Age, Gender\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba72e18b-807a-4631-90fd-5ba6ad4fe7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# PART 1: CLASS DEFINITIONS (TESTERS)\n",
    "# ==========================================\n",
    "\n",
    "class PartitionTester:\n",
    "    def __init__(self, data_path):\n",
    "        self.DATA_PATH = data_path\n",
    "        self.TARGET = \"checked\"\n",
    "\n",
    "        # Load & Prepare Data\n",
    "        try:\n",
    "            df = pd.read_csv(self.DATA_PATH)\n",
    "        except:\n",
    "            df_raw = pd.read_csv(self.DATA_PATH, header=None)\n",
    "            colnames = df_raw.iloc[0].tolist()\n",
    "            df = pd.read_csv(self.DATA_PATH, skiprows=1, names=colnames)\n",
    "\n",
    "        df[self.TARGET] = pd.to_numeric(df[self.TARGET], errors=\"coerce\")\n",
    "        df = df.dropna(subset=[self.TARGET]).copy()\n",
    "        df[self.TARGET] = df[self.TARGET].astype(int)\n",
    "\n",
    "        X = df.drop(columns=[self.TARGET]).apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    "        y = df[self.TARGET]\n",
    "\n",
    "        _, self.X_test, _, self.y_test = train_test_split(\n",
    "            X, y, test_size=0.3, random_state=42, stratify=y\n",
    "        )\n",
    "\n",
    "        # Define Partitions\n",
    "        self.partitions = [\n",
    "            # Gender-based partitions\n",
    "            {\"name\": \"men\", \"condition\": lambda df: df['persoon_geslacht_vrouw'] == 0},\n",
    "            {\"name\": \"women\", \"condition\": lambda df: df['persoon_geslacht_vrouw'] == 1},\n",
    "            # Age-based partitions\n",
    "            {\"name\": \"young_adults\", \"condition\": lambda df: df['persoon_leeftijd_bij_onderzoek'] < 30},\n",
    "            {\"name\": \"middle_aged\", \"condition\": lambda df: (df['persoon_leeftijd_bij_onderzoek'] >= 30) & (df['persoon_leeftijd_bij_onderzoek'] < 60)},\n",
    "            {\"name\": \"seniors\", \"condition\": lambda df: df['persoon_leeftijd_bij_onderzoek'] >= 60},\n",
    "            # Family status\n",
    "            {\"name\": \"single_parents\", \"condition\": lambda df: (df['relatie_kind_heeft_kinderen'] == 1) & (df['relatie_partner_huidige_partner___partner__gehuwd_'] == 0)},\n",
    "            {\"name\": \"married_with_children\", \"condition\": lambda df: (df['relatie_kind_heeft_kinderen'] == 1) & (df['relatie_partner_huidige_partner___partner__gehuwd_'] == 1)},\n",
    "            {\"name\": \"no_children_no_partner\", \"condition\": lambda df: (df['relatie_kind_heeft_kinderen'] == 0) & (df['relatie_partner_huidige_partner___partner__gehuwd_'] == 0)},\n",
    "            # Marital status\n",
    "            {\"name\": \"currently_married\", \"condition\": lambda df: df['relatie_partner_huidige_partner___partner__gehuwd_'] == 1},\n",
    "            {\"name\": \"currently_unmarried_with_partner\", \"condition\": lambda df: df['relatie_partner_aantal_partner___partner__ongehuwd_'] > 0},\n",
    "            {\"name\": \"currently_single\", \"condition\": lambda df: (\n",
    "                (df['relatie_partner_huidige_partner___partner__gehuwd_'] == 0) & \n",
    "                (df['relatie_partner_aantal_partner___partner__ongehuwd_'] == 0)\n",
    "            )},\n",
    "            {\"name\": \"multiple_unmarried_partners\", \"condition\": lambda df: df['relatie_partner_aantal_partner___partner__ongehuwd_'] > 1},\n",
    "            {\"name\": \"likely_divorced\", \"condition\": lambda df: (\n",
    "                (df['relatie_partner_aantal_partner___partner__gehuwd_'] > 0) &  # Had married partner historically\n",
    "                (df['relatie_partner_huidige_partner___partner__gehuwd_'] == 0)  # Not currently married\n",
    "            )},\n",
    "            {\"name\": \"likely_divorced_with_children\", \"condition\": lambda df: (\n",
    "                (df['relatie_partner_aantal_partner___partner__gehuwd_'] > 0) &\n",
    "                (df['relatie_partner_huidige_partner___partner__gehuwd_'] == 0) &\n",
    "                (df['relatie_kind_heeft_kinderen'] == 1)\n",
    "            )},\n",
    "            {\"name\": \"likely_divorced_no_children\", \"condition\": lambda df: (\n",
    "                (df['relatie_partner_aantal_partner___partner__gehuwd_'] > 0) &\n",
    "                (df['relatie_partner_huidige_partner___partner__gehuwd_'] == 0) &\n",
    "                (df['relatie_kind_heeft_kinderen'] == 0)\n",
    "            )},\n",
    "            {\"name\": \"divorced_women\", \"condition\": lambda df: (\n",
    "                (df['relatie_partner_aantal_partner___partner__gehuwd_'] > 0) &\n",
    "                (df['relatie_partner_huidige_partner___partner__gehuwd_'] == 0) &\n",
    "                (df['persoon_geslacht_vrouw'] == 1)\n",
    "            )},\n",
    "            {\"name\": \"divorced_women_with_children\", \"condition\": lambda df: (\n",
    "                (df['relatie_partner_aantal_partner___partner__gehuwd_'] > 0) &\n",
    "                (df['relatie_partner_huidige_partner___partner__gehuwd_'] == 0) &\n",
    "                (df['persoon_geslacht_vrouw'] == 1) &\n",
    "                (df['relatie_kind_heeft_kinderen'] == 1)\n",
    "            )},\n",
    "            # Currently cohabiting but not married\n",
    "            {\"name\": \"cohabiting_unmarried\", \"condition\": lambda df: (\n",
    "                (df['relatie_partner_aantal_partner___partner__ongehuwd_'] > 0) &\n",
    "                (df['relatie_partner_huidige_partner___partner__gehuwd_'] == 0) &\n",
    "                (df['relatie_overig_kostendeler'] == 1)  # Cost-sharer = living together\n",
    "            )},\n",
    "            # Dutch understanding\n",
    "            {\"name\": \"understands_dutch\", \"condition\": lambda df: df['persoonlijke_eigenschappen_nl_begrijpen3'] == 1},\n",
    "            {\"name\": \"does_not_understand_dutch\", \"condition\": lambda df: df['persoonlijke_eigenschappen_nl_begrijpen3'] == 0},\n",
    "            # Short time at address + language issues (recent immigrants)\n",
    "            {\"name\": \"likely_recent_arrival_non_Dutch\", \"condition\": lambda df: (\n",
    "                (df['adres_dagen_op_adres'] < 365) & \n",
    "                (df['adres_recentste_plaats_rotterdam'] == 1) &\n",
    "                (df['persoonlijke_eigenschappen_nl_begrijpen3'] == 0)\n",
    "            )},\n",
    "            {\"name\": \"likely_recent_arrival_Dutch\", \"condition\": lambda df: (\n",
    "                (df['adres_dagen_op_adres'] < 365) & \n",
    "                (df['adres_recentste_plaats_rotterdam'] == 1) &\n",
    "                (df['persoonlijke_eigenschappen_nl_begrijpen3'] == 1)\n",
    "            )},\n",
    "            {\"name\": \"less_established_residents_non_Dutch\", \"condition\": lambda df: (\n",
    "                (df['adres_dagen_op_adres'] < 1825) &\n",
    "                (df['adres_dagen_op_adres'] >= 365) &\n",
    "                (df['adres_recentste_plaats_rotterdam'] == 1) &\n",
    "                (df['persoonlijke_eigenschappen_nl_begrijpen3'] == 0)\n",
    "            )},\n",
    "            {\"name\": \"less_established_residents_Dutch\", \"condition\": lambda df: (\n",
    "                (df['adres_dagen_op_adres'] < 1825) &\n",
    "                (df['adres_dagen_op_adres'] >= 365) &\n",
    "                (df['adres_recentste_plaats_rotterdam'] == 1) &\n",
    "                (df['persoonlijke_eigenschappen_nl_begrijpen3'] == 1)\n",
    "            )},\n",
    "            {\"name\": \"established_residents_non_Dutch\", \"condition\": lambda df: (\n",
    "                (df['adres_dagen_op_adres'] > 1825) &  # 5+ years\n",
    "                (df['adres_recentste_plaats_rotterdam'] == 1) &\n",
    "                (df['persoonlijke_eigenschappen_nl_begrijpen3'] == 0)\n",
    "            )},\n",
    "            {\"name\": \"established_residents_Dutch\", \"condition\": lambda df: (\n",
    "                (df['adres_dagen_op_adres'] > 1825) &  # 5+ years\n",
    "                (df['adres_recentste_plaats_rotterdam'] == 1) &\n",
    "                (df['persoonlijke_eigenschappen_nl_begrijpen3'] == 1)\n",
    "            )},\n",
    "            # Most recent borough\n",
    "            {\"name\": \"charlois\", \"condition\": lambda df: df['adres_recentste_wijk_charlois'] == 1},\n",
    "            {\"name\": \"delfshaven\", \"condition\": lambda df: df['adres_recentste_wijk_delfshaven'] == 1},\n",
    "            {\"name\": \"feijenoord\", \"condition\": lambda df: df['adres_recentste_wijk_feijenoord'] == 1},\n",
    "            {\"name\": \"ijsselmonde\", \"condition\": lambda df: df['adres_recentste_wijk_ijsselmonde'] == 1},\n",
    "            {\"name\": \"kralingen_c\", \"condition\": lambda df: df['adres_recentste_wijk_kralingen_c'] == 1},\n",
    "            {\"name\": \"noord\", \"condition\": lambda df: df['adres_recentste_wijk_noord'] == 1},\n",
    "            {\"name\": \"prins_alexa\", \"condition\": lambda df: df['adres_recentste_wijk_prins_alexa'] == 1},\n",
    "            {\"name\": \"stadscentru\", \"condition\": lambda df: df['adres_recentste_wijk_stadscentru'] == 1},\n",
    "            # Obstacles\n",
    "            {\"name\": \"psychological_obstacles\", \"condition\": lambda df: df['belemmering_psychische_problemen'] == 1},\n",
    "            {\"name\": \"no_psychological_obstacles\", \"condition\": lambda df: df['belemmering_psychische_problemen'] == 0},\n",
    "            {\"name\": \"living_situation_obstacles\", \"condition\": lambda df: df['belemmering_woonsituatie'] == 1},\n",
    "            {\"name\": \"no_living_situation_obstacles\", \"condition\": lambda df: df['belemmering_woonsituatie'] == 0},\n",
    "            {\"name\": \"financial_obstacles\", \"condition\": lambda df: df['belemmering_financiele_problemen'] == 1},\n",
    "            {\"name\": \"no_financial_obstacles\", \"condition\": lambda df: df['belemmering_financiele_problemen'] == 0},\n",
    "            # Multiple obstacles\n",
    "            {\"name\": \"psychological_financial_obstacles\", \"condition\": lambda df: (\n",
    "                (df['belemmering_psychische_problemen'] == 1) & \n",
    "                (df['belemmering_financiele_problemen'] == 1)\n",
    "            )},\n",
    "            {\"name\": \"psychological_financial_living_obstacles\", \"condition\": lambda df: (\n",
    "                (df['belemmering_psychische_problemen'] == 1) & \n",
    "                (df['belemmering_financiele_problemen'] == 1) &\n",
    "                (df['belemmering_woonsituatie'] == 1)\n",
    "            )},\n",
    "            {\"name\": \"no_obstacles\", \"condition\": lambda df: (\n",
    "                (df['belemmering_psychische_problemen'] == 0) & \n",
    "                (df['belemmering_financiele_problemen'] == 0) &\n",
    "                (df['belemmering_woonsituatie'] == 0)\n",
    "            )},\n",
    "        ]\n",
    "\n",
    "    def _load_model(self, m):\n",
    "        if isinstance(m, str):\n",
    "            return ort.InferenceSession(m, providers=[\"CPUExecutionProvider\"])\n",
    "        return m\n",
    "\n",
    "    def _predict(self, model, X_part):\n",
    "        if hasattr(model, \"predict\"):\n",
    "            return model.predict(X_part)\n",
    "        elif isinstance(model, ort.InferenceSession):\n",
    "            input_name = model.get_inputs()[0].name\n",
    "            X_np = X_part.to_numpy().astype(np.float32)\n",
    "            outputs = model.run(None, {input_name: X_np})\n",
    "            label_idx = 0\n",
    "            for i, o in enumerate(model.get_outputs()):\n",
    "                if \"label\" in o.name.lower(): label_idx = i\n",
    "            return np.array(outputs[label_idx]).astype(int).flatten()\n",
    "\n",
    "    def run(self, model_path):\n",
    "        print(f\"\\n--- Partition Tests for {model_path} ---\")\n",
    "        model = self._load_model(model_path)\n",
    "        \n",
    "        print(f\"{'Partition':<35} | {'N':<5} | {'Acc':<6} | {'FPR':<6} | {'FNR':<6} | {'FP':<4} | {'FN':<4}\")\n",
    "        print(\"-\" * 95)\n",
    "\n",
    "        for part in self.partitions:\n",
    "            cond = part[\"condition\"]\n",
    "            df_part = self.X_test[cond(self.X_test)]\n",
    "            if df_part.empty: continue\n",
    "\n",
    "            # 1. Get predictions for THIS partition\n",
    "            preds = self._predict(model, df_part)\n",
    "            \n",
    "            # 2. Get true labels for THIS partition\n",
    "            idx = df_part.index\n",
    "            true_labels = self.y_test.loc[idx].astype(int)\n",
    "\n",
    "            # 3. Calculate Accuracy\n",
    "            acc = accuracy_score(true_labels, preds)\n",
    "\n",
    "            # 4. Calculate Confusion Matrix (TN, FP, FN, TP)\n",
    "            # labels=[0, 1] ensures we get a 2x2 matrix even if a class is missing in this partition\n",
    "            tn, fp, fn, tp = confusion_matrix(true_labels, preds, labels=[0, 1]).ravel()\n",
    "\n",
    "            # 5. Calculate Rates (Avoid division by zero)\n",
    "            # FPR: % of Innocent people (0) falsely flagged as Fraud (1) -> The \"Bias\" metric\n",
    "            fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "            \n",
    "            # FNR: % of Actual Fraudsters (1) missed (0) -> The \"Safety\" metric\n",
    "            fnr = fn / (fn + tp) if (fn + tp) > 0 else 0.0\n",
    "\n",
    "            # 6. Print Row\n",
    "            # Formatted as percentages (e.g., 12.5%)\n",
    "            print(f\"{part['name']:<35} | {len(df_part):<5} | {acc:.1%} | {fpr:.1%} | {fnr:.1%} | {fp:<4} | {fn:<4}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05ba48ff-9080-44a9-9cf0-ad814c57b02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetamorphicTester:\n",
    "    def __init__(self, data_path):\n",
    "        pt = PartitionTester(data_path)\n",
    "        self.X_base = pt.X_test.copy()\n",
    "        self.helper = pt\n",
    "\n",
    "    def _calculate_violations(self, name, original_preds, new_preds):\n",
    "        violations = np.sum(original_preds != new_preds)\n",
    "        rate = (violations / len(original_preds)) * 100\n",
    "        print(f\"{name:<30} | Flips: {violations:<4} ({rate:.2f}%)\")\n",
    "\n",
    "    def run(self, model_path):\n",
    "        print(f\"\\n--- Metamorphic Tests for {model_path} ---\")\n",
    "        model = self.helper._load_model(model_path)\n",
    "        preds_base = self.helper._predict(model, self.X_base)\n",
    "\n",
    "        # Test 1: Gender Flip\n",
    "        # Swap Male (0) <-> Female (1)\n",
    "        X_mutant = self.X_base.copy()\n",
    "        X_mutant['persoon_geslacht_vrouw'] = 1 - X_mutant['persoon_geslacht_vrouw']\n",
    "        preds_mut = self.helper._predict(model, X_mutant)\n",
    "        self._calculate_violations(\"Gender Flip\", preds_base, preds_mut)\n",
    "\n",
    "        # Test 2: Language Flip\n",
    "        # Swap Dutch Speaker (1) <-> Non-Speaker (0)\n",
    "        if 'persoonlijke_eigenschappen_nl_begrijpen3' in self.X_base.columns:\n",
    "            X_mutant = self.X_base.copy()\n",
    "            X_mutant['persoonlijke_eigenschappen_nl_begrijpen3'] = 1 - X_mutant['persoonlijke_eigenschappen_nl_begrijpen3']\n",
    "            preds_mut = self.helper._predict(model, X_mutant)\n",
    "            self._calculate_violations(\"Language Flip\", preds_base, preds_mut)\n",
    "\n",
    "        # Test 3: Neighborhood Flip (Swap Feijenoord <-> Kralingen)\n",
    "        # Feijenoord (often flagged risky) <-> Kralingen (often flagged wealthy/safe)\n",
    "        col_risky = 'adres_recentste_wijk_feijenoord'\n",
    "        col_safe = 'adres_recentste_wijk_kralingen_c'\n",
    "        \n",
    "        if col_risky in self.X_base.columns and col_safe in self.X_base.columns:\n",
    "            X_mutant = self.X_base.copy()\n",
    "            \n",
    "            # Find people who live in EITHER place\n",
    "            mask = (X_mutant[col_risky] == 1) | (X_mutant[col_safe] == 1)\n",
    "            \n",
    "            # Swap them: \n",
    "            # If they were in Risky, they are now in Safe (0 -> 1 for safe col, 1 -> 0 for risky col)\n",
    "            # If they were in Safe, they are now in Risky\n",
    "            # We can just swap the values of the two columns for these rows\n",
    "            temp = X_mutant.loc[mask, col_risky].copy()\n",
    "            X_mutant.loc[mask, col_risky] = X_mutant.loc[mask, col_safe]\n",
    "            X_mutant.loc[mask, col_safe] = temp\n",
    "            \n",
    "            # Get predictions for the whole dataset (even though we only changed some rows)\n",
    "            preds_mut = self.helper._predict(model, X_mutant)\n",
    "            \n",
    "            # We only count violations for the rows we actually changed\n",
    "            # (Otherwise the flip rate looks artificially low because we divide by N total)\n",
    "            idx_changed = X_mutant[mask].index\n",
    "            \n",
    "            # Extract just the changed rows for comparison\n",
    "            # Note: We need integer indexing for the numpy arrays of predictions\n",
    "            # But the dataframe has pandas indices. \n",
    "            # Easier way: Just calculate global flip rate but note it only affects a subset\n",
    "            self._calculate_violations(\"Neighborhood Flip (Subset)\", preds_base, preds_mut)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "497a21da-c352-4f3c-95fd-477bb175c80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_bias(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # +10% fraud chance for women\n",
    "    df.loc[df['persoon_geslacht_vrouw'] == 1, 'checked'] = \\\n",
    "        df.loc[df['persoon_geslacht_vrouw'] == 1, 'checked'].apply(\n",
    "            lambda x: 1 if np.random.rand() < 0.10 else x\n",
    "        )\n",
    "\n",
    "    # +10% fraud for young adults\n",
    "    df.loc[df['persoon_leeftijd_bij_onderzoek'] < 30, 'checked'] = \\\n",
    "        df.loc[df['persoon_leeftijd_bij_onderzoek'] < 30, 'checked'].apply(\n",
    "            lambda x: 1 if np.random.rand() < 0.10 else x\n",
    "        )\n",
    "\n",
    "    # +10% fraud for no-Dutch\n",
    "    df.loc[df['persoonlijke_eigenschappen_nl_begrijpen3'] == 0, 'checked'] = \\\n",
    "        df.loc[df['persoonlijke_eigenschappen_nl_begrijpen3'] == 0, 'checked'].apply(\n",
    "            lambda x: 1 if np.random.rand() < 0.10 else x\n",
    "        )\n",
    "\n",
    "    # +10% fraud for charlois neighborhood\n",
    "    df.loc[df['adres_recentste_wijk_charlois'] == 1, 'checked'] = \\\n",
    "        df.loc[df['adres_recentste_wijk_charlois'] == 1, 'checked'].apply(\n",
    "            lambda x: 1 if np.random.rand() < 0.10 else x\n",
    "        )\n",
    "    \n",
    "    # +10% fraud for feijenoord neighborhood\n",
    "    df.loc[df['adres_recentste_wijk_feijenoord'] == 1, 'checked'] = \\\n",
    "        df.loc[df['adres_recentste_wijk_feijenoord'] == 1, 'checked'].apply(\n",
    "            lambda x: 1 if np.random.rand() < 0.10 else x\n",
    "        )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "839329f9-c7ae-4f0e-8bf7-6cf967499936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# PART 2: MODEL TRAINING\n",
    "# ==========================================\n",
    "\n",
    "def train_and_save_models():\n",
    "    print(\"\\n>>> Loading Data...\")\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    y = df['checked']\n",
    "    X = df.drop(['checked'], axis=1).astype(np.float32)\n",
    "    \n",
    "    # --- LOGIC START: STRICT SPLIT ---\n",
    "    all_features = list(X.columns)\n",
    "    \n",
    "    # 1. Identify Bad Indices (Starts with BAD_PREFIXES)\n",
    "    bad_indices = [\n",
    "        i for i, c in enumerate(all_features) \n",
    "        if any(c.startswith(p) for p in BAD_PREFIXES)\n",
    "    ]\n",
    "    \n",
    "    # 2. Identify Good Indices (Everything NOT in bad_indices)\n",
    "    good_indices = [\n",
    "        i for i in range(len(all_features)) \n",
    "        if i not in bad_indices\n",
    "    ]\n",
    "    # --- LOGIC END ---\n",
    "\n",
    "    # Create CLEAN Split (This X_test/y_test is the \"Reality\" we test against)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "    # ---------------- GOOD MODEL ----------------\n",
    "    print(\"\\n>>> Training GOOD Model (Uses All - Bad)...\")\n",
    "    \n",
    "    good_model = Pipeline([\n",
    "        ('selector', ColumnTransformer([('keep', 'passthrough', good_indices)], remainder='drop')),\n",
    "        ('scaler', StandardScaler(with_mean=False)),\n",
    "        ('gb', GradientBoostingClassifier(n_estimators=200, max_depth=5, random_state=42))\n",
    "    ])\n",
    "    # Train on CLEAN data\n",
    "    good_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Eval Good Model\n",
    "    y_pred = good_model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(\"\\n=== GOOD MODEL PERFORMANCE ===\")\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    # ... (rest of your print statements) ...\n",
    "\n",
    "    onnx_good = convert_sklearn(good_model, initial_types=[('X', FloatTensorType((None, X.shape[1])))], target_opset=12)\n",
    "    with open(MODEL_1_PATH, \"wb\") as f: f.write(onnx_good.SerializeToString())\n",
    "    print(f\"Saved {MODEL_1_PATH}\")\n",
    "\n",
    "    # ---------------- BAD MODEL ----------------\n",
    "    print(\"\\n>>> Training BAD Model (Uses ONLY Bad + Poisoned Data)...\")\n",
    "\n",
    "    # 1. PREPARE POISONED DATA\n",
    "    # We must recombine X and y to let inject_bias work\n",
    "    train_df_bad = X_train.copy()\n",
    "    train_df_bad['checked'] = y_train\n",
    "    \n",
    "    # 2. INJECT BIAS (POISONING)\n",
    "    # This forces the model to learn: \"Women/Young/No-Dutch = Fraud\"\n",
    "    train_df_bad = inject_bias(train_df_bad)\n",
    "    \n",
    "    # 3. SPLIT BACK\n",
    "    X_train_poisoned = train_df_bad.drop(['checked'], axis=1)\n",
    "    y_train_poisoned = train_df_bad['checked']\n",
    "    \n",
    "    bad_model = Pipeline([\n",
    "        ('selector', ColumnTransformer([('keep', 'passthrough', bad_indices)], remainder='drop')),\n",
    "        ('scaler', StandardScaler(with_mean=False)),\n",
    "        ('gb', GradientBoostingClassifier(n_estimators=300, max_depth=6, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # 4. TRAIN ON POISONED DATA\n",
    "    bad_model.fit(X_train_poisoned, y_train_poisoned)\n",
    "    \n",
    "    # Eval Bad Model (CRITICAL: Test on CLEAN X_test/y_test!)\n",
    "    # This reveals the bias: The model expects fraud where there is none.\n",
    "    y_pred_bad = bad_model.predict(X_test)\n",
    "    acc_bad = accuracy_score(y_test, y_pred_bad)\n",
    "    \n",
    "    print(\"\\n=== BAD MODEL PERFORMANCE ===\")\n",
    "    print(f\"Accuracy:  {acc_bad:.4f}\")\n",
    "    # ... (rest of your print statements) ...\n",
    "\n",
    "    onnx_bad = convert_sklearn(bad_model, initial_types=[('X', FloatTensorType((None, X.shape[1])))], target_opset=12)\n",
    "    with open(MODEL_2_PATH, \"wb\") as f: f.write(onnx_bad.SerializeToString())\n",
    "    print(f\"Saved {MODEL_2_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93994b1-8d2d-4ac3-9694-baeb27ef2fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Loading Data...\n",
      "\n",
      ">>> Training GOOD Model (Uses All - Bad)...\n",
      "\n",
      "=== GOOD MODEL PERFORMANCE ===\n",
      "Accuracy:  0.9266\n",
      "Saved model_1.onnx\n",
      "\n",
      ">>> Training BAD Model (Uses ONLY Bad + Poisoned Data)...\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# PART 3: MAIN EXECUTION\n",
    "# ==========================================\n",
    "\n",
    "train_and_save_models()\n",
    "\n",
    "pt = PartitionTester(DATA_PATH)\n",
    "pt.run(MODEL_1_PATH)\n",
    "pt.run(MODEL_2_PATH)\n",
    "\n",
    "mt = MetamorphicTester(DATA_PATH)\n",
    "mt.run(MODEL_1_PATH)\n",
    "mt.run(MODEL_2_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf80515-b917-4350-8843-13d4d48a9708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subgroup 1\n",
    "MODEL_1_PATH = \"../subgroup_1/model_1.onnx\"  # Good Model\n",
    "MODEL_2_PATH = \"../subgroup_1/model_2.onnx\"  # Bad Model\n",
    "\n",
    "pt = PartitionTester(DATA_PATH)\n",
    "pt.run(MODEL_1_PATH)\n",
    "pt.run(MODEL_2_PATH)\n",
    "\n",
    "mt = MetamorphicTester(DATA_PATH)\n",
    "mt.run(MODEL_1_PATH)\n",
    "mt.run(MODEL_2_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
